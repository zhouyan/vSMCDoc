\documentclass[11pt,bib,mint,hyper,altcolor]{marticle}

\addbibresource{user_guide.bib}
\newlength\fixedchar
\settowidth{\fixedchar}{\texttt{\normalsize 0}}
\geometry{textwidth=80\fixedchar}

\newmintedfile{cmake}{bgcolor=SBase3}
\newmintedfile{cpp}{bgcolor=SBase3}
\newmintedfile{c}{bgcolor=SBase3}
\newmintedfile{r}{bgcolor=SBase3}
\newmintedfile{sh}{bgcolor=SBase3}
\newmintedfile{text}{bgcolor=SBase3}
\newminted{cmake}{bgcolor=SBase3}
\newminted{cpp}{bgcolor=SBase3}
\newminted{c}{bgcolor=SBase3}
\newminted{r}{bgcolor=SBase3}
\newminted{sh}{bgcolor=SBase3}
\newminted{text}{bg=SBase3}
\newmintinline{cmake}{}
\newmintinline{cpp}{}
\newmintinline{c}{}
\newmintinline{r}{}
\newmintinline{sh}{}
\newmintinline{text}{}

\UseAbbr{ais}
\UseAbbr{blas}
\UseAbbr{cpu}
\UseAbbr{ess}
\UseAbbr{gpu}
\UseAbbr{lapack}
\UseAbbr{mkl}
\UseAbbr{mpi}
\UseAbbr{rng}
\UseAbbr{simd}
\UseAbbr{sis}
\UseAbbr{smc}

\UseAbbr[\cpp][\textcase]{C++}
\UseAbbr[\cppoo][\lnfigures\textcase]{C++11}
\UseAbbr[\hdf]{hdf5}
\UseAbbr[\opencl][]{OpenCL}
\UseAbbr[\openmp][]{OpenMP}
\UseAbbr[\vsmc][]{vSMC}

\def\cmake{\href{https://cmake.org}{CMake}\xspace}
\def\tbb{\href{https://www.threadingbuildingblocks.org}{Intel TBB}\xspace}

\begin{document}

\title{\protect\vsmc{} -- Parallel Sequential Monte Carlo in \protect\cpp}
\author{Yan Zhou}
\date{\today}

\maketitle
\tableofcontents

\begin{abstract}
  Sequential Monte Carlo is a family of algorithms for sampling from a sequence
  of distributions. Some of these algorithms, such as particle filters, are
  widely used in the physics and signal processing researches.  More recent
  developments have established their application in more general inference
  problems such as Bayesian modeling.

  These algorithms have attracted considerable attentions in recent years as
  they admit natural and scalable parallelization. However, these algorithms
  are perceived to be difficult to implement. In addition, parallel programming
  is often unfamiliar to many researchers though conceptually appealing,
  especially for sequential Monte Carlo related fields.

  A \cpp template library is presented for the purpose of implementing general
  sequential Monte Carlo algorithms on parallel hardware. Two examples are
  presented: a simple particle filter and a classic Bayesian modeling problem.
\end{abstract}

\section{Introduction}
\label{sec:Introduction}

Sequential Monte Carlo (\smc) methods are a class of sampling algorithms that
combine importance sampling and resampling. They have been primarily used as
``particle filters'' to solve optimal filtering problems; see, for example,
\textcite{Cappe:2007hz} and \textcite{Doucet:2011us} for recent reviews. They
are also used in a static setting where a target distribution is of interest,
for example, for the purpose of Bayesian modeling. This was proposed by
\textcite{DelMoral:2006hc} and developed by \textcite{Peters:2005wh} and
\textcite{DelMoral:2006wv}. This framework involves the construction of a
sequence of artificial distributions on spaces of increasing dimensions which
admit the distributions of interest as particular marginals.

\smc algorithms are perceived as being difficult to implement while general
tools were not available until the development by \textcite{Johansen:2009wd},
which provided a general framework for implementing \smc algorithms. \smc
algorithms admit natural and scalable parallelization. However, there are only
parallel implementations of \smc algorithms for many problem specific
applications, usually associated with specific \smc related researches.
\textcite{Lee:2010fm} studied the parallelization of \smc algorithms on \gpu{}s
with some generality. There are few general tools to implement \smc algorithms
on parallel hardware though multicore \cpu{}s are very common today and
computing on specialized hardware such as \gpu{}s are more and more popular.

The purpose of the current work is to provide a general framework for
implementing \smc algorithms on both sequential and parallel hardware. There
are two main goals of the presented framework. The first is reusability. It
will be demonstrated that the same implementation source code can be used to
build a serialized sampler, or using different programming models (for example,
\openmp and \tbb) to build parallelized samplers for multicore \cpu{}s. They
can be scaled for clusters using \mpi with few modifications.  And with a
little effort they can also be used to build parallelized samplers on
specialized massive parallel hardware such as \gpu{}s using \opencl. The second
is extensibility. It is possible to write a backend for \vsmc to use new
parallel programming models while reusing existing implementations. It is also
possible to enhance the library to improve performance for specific
applications. Almost all components of the library can be reimplemented by
users and thus if the default implementation is not suitable for a specific
application, they can be replaced while being integrated with other components
seamlessly.

\section{Sequential Monte Carlo}
\label{sec:Sequential Monte Carlo}

\subsection{Sequential importance sampling and resampling}
\label{sub:Sequential importance sampling and resampling}

Importance sampling is a technique which allows the calculation of the
expectation of a function $\varphi$ with respect to a distribution $\pi$ using
samples from some other distribution $\eta$ with respect to which $\pi$ is
absolutely continuous, based on the identity,
\begin{equation}
  \Exp_{\pi}[\varphi(X)]
  = \int\varphi(x)\pi(x)\intd x
  = \int\frac{\varphi(x)\pi(x)}{\eta(x)}\eta(x)\intd x
  = \Exp_{\eta}\Bigl[\frac{\varphi(X)\pi(X)}{\eta(X)}\Bigr]
\end{equation}
And thus, let $\{X^{(i)}\}_{i=1}^N$ be samples from $\eta$, then
$\Exp_{\pi}[\varphi(X)]$ can be approximated by
\begin{equation}
  \hat\varphi_1 =
  \frac{1}{N}\sum_{i=1}^N\frac{\varphi(X^{(i)})\pi(X^{(i)})}{\eta(X^{(i)})}
\end{equation}
In practice $\pi$ and $\eta$ are often only known up to some normalizing
constants, which can be estimated using the same samples. Let $w^{(i)} =
\pi(X^{(i)})/\eta(X^{(i)})$, then we have
\begin{equation}
  \hat\varphi_2 =
  \frac{\sum_{i=1}^Nw^{(i)}\varphi(X^{(i)})}{\sum_{i=1}^Nw^{(i)}}
\end{equation}
or
\begin{equation}
  \hat\varphi_3 = \sum_{i=1}^NW^{(i)}\varphi(X^{(i)})
\end{equation}
where $W^{(i)}\propto w^{(i)}$ and are normalized such that
$\sum_{i=1}^NW^{(i)} = 1$.

Sequential importance sampling (\sis) generalizes the importance sampling
technique for a sequence of distributions $\{\pi_t\}_{t\ge0}$ defined on spaces
$\{\prod_{k=0}^tE_k\}_{t\ge0}$. At time $t = 0$, sample $\{X_0^{(i)}\}_{i=1}^N$
from $\eta_0$ and compute the weights $W_0^{(i)} \propto
\pi_0(X_0^{(i)})/\eta_0(X_0^{(i)})$. At time $t\ge1$, each sample
$X_{0:t-1}^{(i)}$, usually termed \emph{particles} in the literature, is
extended to $X_{0:t}^{(i)}$ by a proposal distribution
$q_t(\cdot|X_{0:t-1}^{(i)})$. And the weights are recalculated by $W_t^{(i)}
\propto \pi_t(X_{0:t}^{(i)})/\eta_t(X_{0:t}^{(i)})$ where
\begin{equation}
  \eta_t(X_{0:t}^{(i)}) =
  \eta_{t-1}(X_{0:t-1}^{(i)})q_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)})
\end{equation}
and thus
\begin{align}
  W_t^{(i)} \propto \frac{\pi_t(X_{0:t}^{(i)})}{\eta_t(X_{0:t}^{(i)})}
  &= \frac{\pi_t(X_{0:t}^{(i)})\pi_{t-1}(X_{0:t-1}^{(i)})}
  {\eta_{t-1}(X_{0:t-1}^{(i)})q_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)})
    \pi_{t-1}(X_{0:t-1}^{(i)})} \notag\\
  &= \frac{\pi_t(X_{0:t}^{(i)})}
  {q_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)})\pi_{t-1}(X_{0:t-1}^{(i)})}W_{t-1}^{(i)}
  \label{eq:si}
\end{align}
and importance sampling estimate of $\Exp_{\pi_t}[\varphi_t(X_{0:t})]$ can be
obtained using $\{W_t^{(i)},X_{0:t}^{(i)}\}_{i=1}^N$.

However this approach fails as $t$ becomes large. The weights tend to become
concentrated on a few particles as the discrepancy between $\eta_t$ and $\pi_t$
becomes larger. Resampling techniques are applied such that, a new particle
system $\{\bar{W}_t^{(i)},\bar{X}_{0:t}^{(i)}\}_{i=1}^M$ is obtained with the
property,
\begin{equation}
  \Exp\Bigl[\sum_{i=1}^M\bar{W}_t^{(i)}\varphi_t(\bar{X}_{0:t}^{(i)})\Bigr] =
  \Exp\Bigl[\sum_{i=1}^NW_t^{(i)}\varphi_t(X_{0:t}^{(i)})\Bigr]
  \label{eq:resample}
\end{equation}
In practice, the resampling algorithm is usually chosen such that $M = N$ and
$\bar{W}^{(i)} = 1/N$ for $i=1,\dots,N$. Resampling can be performed at each
time $t$ or adaptively based on some criteria of the discrepancy. One popular
quantity used to monitor the discrepancy is \emph{effective sample size}
(\ess), introduced by \textcite{Liu:1998iu}, defined as
\begin{equation}
  \ess_t = \frac{1}{\sum_{i=1}^N (W_t^{(i)})^2}
\end{equation}
where $\{W_t^{(i)}\}_{i=1}^N$ are the normalized weights. And resampling can be
performed when $\ess\le \alpha N$ where $\alpha\in[0,1]$.

The common practice of resampling is to replicate particles with large weights
and discard those with small weights. In other words, instead of generating a
random sample $\{\bar{X}_{0:t}^{(i)}\}_{i=1}^N$ directly, a random sample of
integers $\{R^{(i)}\}_{i=1}^N$ is generated, such that $R^{(i)} \ge 0$ for $i =
1,\dots,N$ and $\sum_{i=1}^N R^{(i)} = N$. And each particle value
$X_{0:t}^{(i)}$ is replicated for $R^{(i)}$ times in the new particle system.
The distribution of $\{R^{(i)}\}_{i=1}^N$ shall fulfill the requirement of
Equation~\eqref{eq:resample}. One such distribution is a multinomial
distribution of size $N$ and weights $(W_t^{(i)},\dots,W_t^{(N)})$. See
\textcite{Douc:2005wa} for some commonly used resampling algorithms.

\subsection{\protect\smc samplers}
\label{sub:SMC Samplers}

\smc samplers allow us to obtain, iteratively, collections of weighted samples
from a sequence of distributions $\{\pi_t\}_{t\ge0}$ over essentially any
random variables on some spaces $\{E_t\}_{t\ge0}$, by constructing a sequence
of auxiliary distributions $\{\tilde\pi_t\}_{t\ge0}$ on spaces of increasing
dimensions, $\tilde\pi_t(x_{0:t})=\pi_t (x_t) \prod_{s=0}^{t-1}
L_s(x_{s+1},x_s)$, where the sequence of Markov kernels $\{L_s\}_{s=0}^{t-1}$,
termed backward kernels, is formally arbitrary but critically influences the
estimator variance. See \textcite{DelMoral:2006hc} for further details and
guidance on the selection of these kernels.

Standard sequential importance sampling and resampling algorithms can then be
applied to the sequence of synthetic distributions, $\{\tilde\pi_t\}_{t\ge0}$.
At time $t - 1$, assume that a set of weighted particles
$\{W_{t-1}^{(i)},X_{0:t-1}^{(i)}\}_{i=1}^N$ approximating $\tilde\pi_{t-1}$ is
available, then at time $t$, the path of each particle is extended with a
Markov kernel say, $K_t(x_{t-1}, x_t)$ and the set of particles
$\{X_{0:t}^{(i)}\}_{i=1}^N$ reach the distribution $\eta_t(X_{0:t}^{(i)}) =
\eta_0(X_0^{(i)})\prod_{k=1}^tK_t(X_{t-1}^{(i)}, X_t^{(i)})$, where $\eta_0$ is
the initial distribution of the particles. To correct the discrepancy between
$\eta_t$ and $\tilde\pi_t$, Equation~\eqref{eq:si} is applied and in this case,
\begin{equation}
  W_t^{(i)} \propto \frac{\tilde\pi_t(X_{0:t}^{(i)})}{\eta_t(X_{0:t}^{(i)})}
  = \frac{\pi_t(X_t^{(i)})\prod_{s=0}^{t-1}L_s(X_{s+1}^{(i)}, X_s^{(i)})}
  {\eta_0(X_0^{(i)})\prod_{k=1}^tK_t(X_{t-1}^{(i)},X_t^{(i)})}
  \propto \tilde{w}_t(X_{t-1}^{(i)}, X_t^{(i)})W_{t-1}^{(i)}
\end{equation}
where $\tilde{w}_t$, termed the \emph{incremental weights}, are calculated as,
\begin{equation}
  \tilde{w}_t(X_{t-1}^{(i)},X_t^{(i)}) =
  \frac{\pi_t(X_t^{(i)})L_{t-1}(X_t^{(i)}, X_{t-1}^{(i)})}
  {\pi_{t-1}(X_{t-1}^{(i)})K_t(X_{t-1}^{(i)}, X_t^{(i)})}
\end{equation}
If $\pi_t$ is only known up to a normalizing constant, say $\pi_t(x_t) =
\gamma_t(x_t)/Z_t$, then we can use the \emph{unnormalized} incremental weights
\begin{equation}
  w_t(X_{t-1}^{(i)},X_t^{(i)}) =
  \frac{\gamma_t(X_t^{(i)})L_{t-1}(X_t^{(i)}, X_{t-1}^{(i)})}
  {\gamma_{t-1}(X_{t-1}^{(i)})K_t(X_{t-1}^{(i)}, X_t^{(i)})}
\end{equation}
for importance sampling. Further, with the previously \emph{normalized} weights
$\{W_{t-1}^{(i)}\}_{i=1}^N$, we can estimate the ratio of normalizing constant
$Z_t/Z_{t-1}$ by
\begin{equation}
  \frac{\hat{Z}_t}{Z_{t-1}} =
  \sum_{i=1}^N W_{t-1}^{(i)}w_t(X_{t-1}^{(i)},X_t^{(i)})
\end{equation}
Sequentially, the normalizing constant between initial distribution $\pi_0$ and
some target $\pi_T$, $T\ge1$ can be estimated. See \textcite{DelMoral:2006hc}
for details on calculating the incremental weights. In practice, when $K_t$ is
invariant to $\pi_t$, and an approximated suboptimal backward kernel
\begin{equation}
  L_{t-1}(x_t, x_{t-1}) = \frac{\pi(x_{t-1})K_t(x_{t-1}, x_t)}{\pi_t(x_t)}
\end{equation}
is used, the unnormalized incremental weights will be
\begin{equation}
  w_t(X_{t-1}^{(i)},X_t^{(i)}) =
  \frac{\gamma_t(X_{t-1}^{(i)})}{\gamma_{t-1}(X_{t-1}^{(i)})}.
  \label{eq:inc_weight_mcmc}
\end{equation}

\subsection{Other sequential Monte Carlo algorithms}
\label{sub:Other sequential Monte Carlo algorithms}

Some other commonly used sequential Monte Carlo algorithms can be viewed as
special cases of algorithms introduced above. The annealed importance sampling
(\ais; \textcite{Neal:2001we}) can be viewed as \smc samplers without
resampling.

Particle filters as seen in the physics and signal processing literature, can
also be interpreted as the sequential importance sampling and resampling
algorithms. See \textcite{Doucet:2011us} for a review of this topic.

\section{Basic usage}
\label{sec:Basic usage}

\subsection{Conventions}
\label{sub:Conventions}

All classes that are accessible to users are within the name space
\cppinline{vsmc}. Class names are in \cppinline{CamelCase} and function names,
free or class methods, are in \cppinline{small_cases}. In the remaining of this
guide, we will omit the \cppinline{vsmc::} name space qualifiers.

\subsection{Getting and installing the library}
\label{sub:Getting and installing the library}

The library is hosted at \href{https://github.com/zhouyan/vSMC}{GitHub}. One
can download the stable
\href{https://github.com/zhouyan/vSMC/releases}{Releases} or get the
development branch from the Git repository. This is a header only template \cpp
library. To install the library just move the contents of the
\shinline{include} directory into a proper place, e.g.,
\shinline{/usr/local/include} on Unix-alike systems.  Alternatively, one can
use \cmake (version 2.8.3 or later required).

\begin{listing}
  \begin{shcode}
    cd /path_to_vSMC_source
    mkdir build
    cd build
    cmake ..
    make install
  \end{shcode}
  \caption{Installing the library}
  \label{lst:install the library}
\end{listing}

One may need administrator permissions to perform the last installation step,
or change the destination using \cmakeinline{-DCMAKE_INSTALL_PREFIX}

This library requires a working \blas/\lapack implemetation, with standard C
interface headers (\cppinline{cblas.h} and \cppinline{lapacke.h}).

This library has no other dependencies other than \cpp standard libraries
(\cppoo). Any \cppoo language features are

\subsection{Concepts}
\label{sub:Concepts}

\subsubsection{Sampler}
\label{ssub:Sampler}

\subsubsection{Particle}
\label{ssub:Particle}

\subsubsection{State}
\label{ssub:State}

\subsubsection{Weight}
\label{ssub:Weight}

\subsubsection{Single particle}
\label{ssub:Single particle}

\subsubsection{Monitor}
\label{ssub:Monitor}

\subsection{A simple particle filter}
\label{sub:A simple particle filter}

\begin{listing}
  \cppfile{pf.cpp}
  \caption{\protect\texttt{pf.cpp}}
  \label{lst:pf.cpp}
\end{listing}

\begin{listing}
  \cppfile{pf.hpp}
  \caption{\protect\texttt{pf.hpp}}
  \label{lst:pf.hpp}
\end{listing}

\begin{listing}
  \cppfile{pf_const.hpp}
  \caption{\protect\texttt{pf\_const.hpp}}
  \label{lst:pf_const.hpp}
\end{listing}

\begin{listing}
  \cppfile{pf_state.hpp}
  \caption{\protect\texttt{pf\_state.hpp}}
  \label{lst:pf_state.hpp}
\end{listing}

\begin{listing}
  \cppfile{pf_init.hpp}
  \caption{\protect\texttt{pf\_init.hpp}}
  \label{lst:pf_init.hpp}
\end{listing}

\begin{listing}
  \cppfile{pf_move.hpp}
  \caption{\protect\texttt{pf\_move.hpp}}
  \label{lst:pf_move.hpp}
\end{listing}

\begin{listing}
  \cppfile{pf_meval.hpp}
  \caption{\protect\texttt{pf\_meval.hpp}}
  \label{lst:pf_meval.hpp}
\end{listing}

\subsection{Symmetric Multiprocessing}
\label{sub:Symmetric Multiprocessing}

\subsection{A simple particle filter parallelized}
\label{sub:A simple particle filter parallelized}

\section{Mathematical operations}
\label{sec:Mathemtical operations}

\subsection{Constants}
\label{sub:Constants}

\begin{table}
  \begin{tabu}{X[2l]X[l]X[2l]X[l]X[2l]X[l]}
    \toprule
    Function & Value &
    Function & Value &
    Function & Value \\
    \midrule
    \texttt{pi}             & $\pi$              &
    \texttt{pi\_2}          & $2\pi$             &
    \texttt{pi\_inv}        & $1/\pi$            \\
    \texttt{pi\_sqr}        & $\pi^2$            &
    \texttt{pi\_by2}        & $\pi/2$            &
    \texttt{pi\_by3}        & $\pi/3$            \\
    \texttt{pi\_by4}        & $\pi/4$            &
    \texttt{pi\_by6}        & $\pi/6$            &
    \texttt{pi\_2by3}       & $2\pi/3$           \\
    \texttt{pi\_3by4}       & $3\pi/4$           &
    \texttt{pi\_4by3}       & $4\pi/3$           &
    \texttt{sqrt\_pi}       & $\sqrt{\pi}$       \\
    \texttt{sqrt\_pi\_2}    & $\sqrt{2\pi}$      &
    \texttt{sqrt\_pi\_inv}  & $\sqrt{1/\pi}$     &
    \texttt{sqrt\_pi\_by2}  & $\sqrt{\pi/2}$     \\
    \texttt{sqrt\_pi\_by3}  & $\sqrt{\pi/3}$     &
    \texttt{sqrt\_pi\_by4}  & $\sqrt{\pi/4}$     &
    \texttt{sqrt\_pi\_by6}  & $\sqrt{\pi/6}$     \\
    \texttt{sqrt\_pi\_2by3} & $\sqrt{2\pi/3}$    &
    \texttt{sqrt\_pi\_3by4} & $\sqrt{3\pi/4}$    &
    \texttt{sqrt\_pi\_4by3} & $\sqrt{4\pi/3}$    \\
    \texttt{ln\_pi}         & $\ln{\pi}$         &
    \texttt{ln\_pi\_2}      & $\ln{2\pi}$        &
    \texttt{ln\_pi\_inv}    & $\ln{1/\pi}$       \\
    \texttt{ln\_pi\_by2}    & $\ln{\pi/2}$       &
    \texttt{ln\_pi\_by3}    & $\ln{\pi/3}$       &
    \texttt{ln\_pi\_by4}    & $\ln{\pi/4}$       \\
    \texttt{ln\_pi\_by6}    & $\ln{\pi/6}$       &
    \texttt{ln\_pi\_2by3}   & $\ln{2\pi/3}$      &
    \texttt{ln\_pi\_3by4}   & $\ln{3\pi/4}$      \\
    \texttt{ln\_pi\_4by3}   & $\ln{4\pi/3}$      &
    \texttt{e}              & $\EE$              &
    \texttt{e\_inv}         & $1/\EE$            \\
    \texttt{sqrt\_e}        & $\sqrt{\EE}$       &
    \texttt{sqrt\_e\_inv}   & $\sqrt{1/\EE}$     &
    \texttt{sqrt\_2}        & $\sqrt{2}$         \\
    \texttt{sqrt\_3}        & $\sqrt{3}$         &
    \texttt{sqrt\_5}        & $\sqrt{5}$         &
    \texttt{sqrt\_10}       & $\sqrt{10}$        \\
    \texttt{sqrt\_1by2}     & $\sqrt{1/2}$       &
    \texttt{sqrt\_1by3}     & $\sqrt{1/3}$       &
    \texttt{sqrt\_1by5}     & $\sqrt{1/5}$       \\
    \texttt{sqrt\_1by10}    & $\sqrt{1/10}$      &
    \texttt{ln\_2}          & $\ln{2}$           &
    \texttt{ln\_3}          & $\ln{3}$           \\
    \texttt{ln\_5}          & $\ln{5}$           &
    \texttt{ln\_10}         & $\ln{10}$          &
    \texttt{ln\_inv\_2}     & $1/\ln{2}$         \\
    \texttt{ln\_inv\_3}     & $1/\ln{3}$         &
    \texttt{ln\_inv\_5}     & $1/\ln{5}$         &
    \texttt{ln\_inv\_10}    & $1/\ln{10}$        \\
    \texttt{ln\_ln\_2}      & $\ln\ln{2}$        &
    &                    &
    &                    \\
    \bottomrule
  \end{tabu}
  \caption[Mathematical constants]{Mathematical constants. Note: All functions
    are prefixed by \cppinline{const\_}.}
  \label{tab:Mathematical constants}
\end{table}

\subsection{Vectorized operations}
\label{sub:Vectorized operations}

\section{Resample}
\label{sec:Resample}

\section{Random number generating}
\label{sec:Random number generating}

\subsection{Seeding}
\label{sub:Seeding}

\subsection{Counter based \protect\rng}
\label{sub:Coutner based RNG}

\begin{table}
  \def\B{\textcolor{MRed}{\textit{B}}}
  \def\V{\textcolor{MRed}{\textit{V}}}
  \begin{tabu}{X[2l]X[l]X[l]X[l]}
    \toprule
    Class & \texttt{result\_type} & Counter bits & Key bits \\
    \midrule
    \texttt{AES128\_\B x32} & \texttt{std::uint32\_t} & $128$ & $128$ \\
    \texttt{AES128\_\B x64} & \texttt{std::uint64\_t} & $128$ & $128$ \\
    \texttt{AES192\_\B x32} & \texttt{std::uint32\_t} & $128$ & $192$ \\
    \texttt{AES192\_\B x64} & \texttt{std::uint64\_t} & $128$ & $192$ \\
    \texttt{AES256\_\B x32} & \texttt{std::uint32\_t} & $128$ & $256$ \\
    \texttt{AES256\_\B x64} & \texttt{std::uint64\_t} & $128$ & $256$ \\
    \texttt{ARS\_\B x32}    & \texttt{std::uint32\_t} & $128$ & $128$ \\
    \texttt{ARS\_\B x64}    & \texttt{std::uint64\_t} & $128$ & $128$ \\
    \texttt{Philox2x32\V}   & \texttt{std::uint32\_t} & $64$  & $64$  \\
    \texttt{Philox2x64\V}   & \texttt{std::uint64\_t} & $128$ & $128$ \\
    \texttt{Philox4x32\V}   & \texttt{std::uint32\_t} & $128$ & $128$ \\
    \texttt{Philox4x64\V}   & \texttt{std::uint64\_t} & $256$ & $256$ \\
    \texttt{Threefry2x32\V} & \texttt{std::uint32\_t} & $64$  & $64$  \\
    \texttt{Threefry2x64\V} & \texttt{std::uint64\_t} & $128$ & $128$ \\
    \texttt{Threefry4x32\V} & \texttt{std::uint32\_t} & $128$ & $128$ \\
    \texttt{Threefry4x64\V} & \texttt{std::uint64\_t} & $256$ & $256$ \\
    \bottomrule
  \end{tabu}
  \caption[Counter based \protect\rng]{Counter based \rng; \B: either
    \cppinline{1}, \cppinline{2}, \cppinline{4}, or \cppinline{8}; \V: either
    empty, \cppinline{SSE2}, or \cppinline{AVX2}.}
  \label{tab:Counter based rng}
\end{table}

\subsection{Non-deterministic \protect\rng}
\label{sub:Non-deterministic RNG}

\subsection{Intel \protect\mkl{} \protect\rng}
\label{sub:Intel MKL RNG}

\begin{table}
  \begin{tabu}{X[l]X[l]}
    \toprule
    Class & Intel \mkl \texttt{BRNG} \\
    \midrule
    \texttt{MKL\_MCG59}         & \texttt{VSL\_BRNG\_MCG59}         \\
    \texttt{MKL\_MT19937}       & \texttt{VSL\_BRNG\_MT19937}       \\
    \texttt{MKL\_MT2203}        & \texttt{VSL\_BRNG\_MT2203}        \\
    \texttt{MKL\_SFMT19937}     & \texttt{VSL\_BRNG\_SFMT19937}     \\
    \texttt{MKL\_NONDETERM}     & \texttt{VSL\_BRNG\_NONDETERM}     \\
    \texttt{MKL\_ARS5}          & \texttt{VSL\_BRNG\_ARS5}          \\
    \texttt{MKL\_PHILOX4X32X10} & \texttt{VSL\_BRNG\_PHILOX4X32X10} \\
    \bottomrule
  \end{tabu}
  \caption[Intel \protect\mkl{} \protect\rng]{Intel \mkl{} \rng. Note, all
    classes can have a suffix \cppinline{_64}}.
  \label{tab:Intel MKL RNG}
\end{table}

\subsection{Multiple \protect\rng streams}
\label{sub:Multiple RNG streams}

\subsection{Distributions}
\label{sub:Distributions}

\subsubsection{Uniform bits distributions}
\label{ssub:Uniform bits distributions}

\subsubsection{Standard uniform distributions}
\label{ssub:Standard uniform distributions}

\subsection{Ordered standard uniform random numbers}
\label{sub:Ordered standard uniform random numbers}

\subsubsection{Continuous distributions}
\label{ssub:Continuous distributions}

\subsection{Random walk}
\label{sub:Random walk}

\section{Utilities}
\label{sec:Utilities}

\subsection{Aligned memory allocation}
\label{sub:Aligned memory allocation}

\subsection{Sample covariance estimating}
\label{sub:Sample covariance estimating}

\subsection{Storing objects in \protect\hdf}
\label{sub:Storing objects in hdf}

\subsection{Smart pointers for Intel \protect\mkl objects}
\label{sub:Smart pointers for Intel MKL objects}

\subsection{Program options}
\label{sub:Program options}

\subsection{Program progress}
\label{sub:Program progress}

\subsection{x86 \protect\simd operations}
\label{sub:x86 SIMD operations}

\subsection{Timing}
\label{sub:Timing}

\printbibliography[title=\refname]

\end{document}
